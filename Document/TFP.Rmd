---
title: "TFP"
output: pdf_document
date: "2024-01-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading_TFP_DB, echo=FALSE, include=FALSE}
library(dplyr)
library(tidyr)
library(readxl)

data_TFP <- read_excel("../Stores/OECD_Multifactor_Productivity.xls")

# Preparing the data
countries <- colnames(data_TFP)[-1]

# Vector of continents
continents <- c(
"Australia" = "Oceania",
"Austria" = "Europe",
"Belgium" = "Europe",
"Canada" = "North America",
"Denmark" = "Nordic Region",
"Finland" = "Nordic Region",
"France" = "Europe",
"Germany" = "Europe",
"Greece" = "Europe",
"Ireland" = "Europe",
"Israel" = "Asia",
"Italy" = "Europe",
"Japan" = "Asia",
"Korea" = "Asia",
"Luxemburg" = "Europe",
"Netherlands" = "Europe",
"New Zeland" = "Oceania",
"Norway" = "Nordic Region",
"Portugal" = "Europe",
"Spain" = "Europe",
"Sweden" = "Nordic Region",
"Switzerland" = "Europe",
"United Kingdom" = "Europe",
"United States" = "North America"
)

# Create a data frame for country-continent pairs
country_continent <- data.frame(country = countries, continent = I(continents), id = seq_along(countries))

# Reshape data frame
data_TFP_long <- data_TFP %>%
  pivot_longer(-1, names_to = "country", values_to = "TFP")

# Merge data frame with continents
data_TFP_merged <- merge(data_TFP_long, country_continent, by = "country")

# Group data by continent, year, and calculate the mean TFP
avg_TFP <- data_TFP_merged %>%
  group_by(continent, Year) %>%
  summarise(avg_TFP = mean(TFP, na.rm = TRUE))
```

```{r fig4, fig.align='center', fig.cap = "\\label{fig4}Average of Multi Factor Productivity by Continent, 1996 to 2022.", echo = FALSE}
library(ggplot2)
library(dplyr)


# Convert variables
avg_TFP$Year <- as.numeric(avg_TFP$Year)
avg_TFP$continent <- as.factor(avg_TFP$continent)


# Create the graph
ggplot(avg_TFP, aes(x = Year, y = avg_TFP, color = continent)) +
  geom_line() +
  labs(x = "Year",
       y = "Average Total Factor Productivity") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(ncol = 5, direction = "horizontal", title.position = "top", title.hjust = 0.5))

```

# LLMs vs. Artificial General Intelligence (AGI)

## Understanding the difference between LLMs and AGI

Large Language Models (LLMs) and Artificial General Intelligence (AGI), represent distinct approaches to AI.
Understanding their differences and how they link together is crucial in navigating the evolving landscape of AI research and applications.
AGI, as defined by [@Zhou2023PathTM] is a highly autonomous entity with the remarkable capacity to comprehend, learn, and apply knowledge across an extensive array of tasks and domains.
Unlike narrow AI systems, AGI aims to replicate the breadth of human cognitive abilities.
This ambition makes AGI the pinnacle objective within the field of artificial intelligence.
One of the key distinctions between LLMs and AGI lies in their scope of intelligence.
LLMs are specialized, focusing solely on language-related tasks.
Another critical difference is in learning and adaptation.
AGI systems, are designed for continual learning and adaptation.
As described by [@Wang2012Chapter1I] AGI's ability to be generalized on fundamentally new areas.



## Exploring whether AGI is the real General Purpose Technology

The question of whether AGI can be considered the real General Purpose Technology is soon to become more relevant, as AGI has not yet reached its full potential and audience.
While advanced LLM models like OpenAI's ChatGPT have undoubtedly revolutionized the way people work in a wide range of tasks, they represent just a glimpse of what AGI could ultimately achieve.
AGI, with its aspiration to replicate human-like general intelligence across various domains, holds the promise of transcending the limitations of narrow AI and becoming the ultimate tool for problem-solving and innovation.
Paper by [@Mikki2023ArtificialGI] introduces the intriguing idea that achieving AGI may necessitate noncomputable systems.
This concept challenges conventional thinking in AI by encouraging us to expand our understanding of AGI's potential. It suggests that AGI might require unconventional approaches beyond traditional computational frameworks.
While AGI has not yet fully realized its capabilities, it holds the promise of reshaping the technological landscape.
Mikki's proposal of noncomputable systems, when integrated with other development ideas, offers a tantalizing glimpse into the future of AGI, where its transformative power knows no bounds.

# 5. Acknowledging Benefits and Limitations of LLMs as GPT

## Discussing the potential benefits of LLMs as GPT

One of the significant advantages of incorporating LLMs as GPT is the potential for substantial productivity gains.
LLMs excel in natural language understanding and generation, making them valuable tools for automating tasks that involve processing and generating text.
From content generation to customer support automation, LLMs can streamline processes, reduce labor costs, and boost efficiency.
By automating routine language-related tasks, they free up human resources to focus on more creative and complex endeavors.
In fields such as finance, healthcare, and market analysis, LLMs can assist in extracting valuable insights from unstructured data, leading to better-informed decisions and strategies.

## Addressing the limitations and challenges associated with LLMs as GPT

While LLMs offer significant potential as GPT, it is essential to acknowledge the limitations and challenges associated with their widespread adoption.
The use of LLMs raises ethical concerns, particularly in content generation and manipulation.
LLMs rely on large datasets for training, often containing sensitive information.
Safeguarding data privacy and ensuring compliance with data protection regulations is an ongoing challenge that must be addressed to harness the full potential of LLMs.

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
